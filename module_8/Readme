[Car class prediction] 

Эксперименты по построению модели для классификация изображений

Порядок экспериментов:
Model_1: бейзлайн на основе базовой модели Xception
Model_2: заменили base model на FixEfficientNet-B7, но точность стала даже ниже. Продолжим пока с этой моделью и попробуем изменить другие параметры.
Model_3: заменили Image Augmentation метод на Albumentations с различными параметрами, точность упала до 85.7%, но кажется что переобучение сильно сократилось.
Model_4: добавили early stopping callback и увеличили количетсво эпох >> точность упала даже ниже
Model_5: поменяли структуру модели на Sequential() и добавили BatchNormalization
На этом этапе были замечены несколько проблем, которые могли повлечь за собой такой низкий accuracy score. Изменения на этой версии:

убираем аугментацию тестовой выборки
save_best_only = True
также уменьшаем размер изображения на этих тестовых итерациях до 128 чтобы все быстрее считалось и увеличиваем количество эпох до 10
также кажется что на этом этапе лучше сменить base_model чтобы обучение проходило быстрее и дальше докрутить какую-то из моделей когда вернемся хотя бы к изначальному значению точности. FixEfficientNet-B7 использует 66М параметров, заменим его на FixEfficientNet-B5, который использует в два раза меньше (30М).

Model_6: со всеми упомянутыми выше шагами Accuracy rate выросла обратно до 79.69%. Следующее что можно попробовать - убрать слишком большее количество аугментаций, которые мы используем. Насколько я поняла в теории, на меньших количествах эпох должно лучше сработать если мы используем меньше аугментаций. Вместо Albumentations будем использовать стандартные аугментации из ImageDataGenerator.

Model_7: из аугментаций оставим только:

horizontal_flip = 0.3,
rotation_range = 10,
zoom_range = 0.1, и увеличиваем количество эпох до 15. >> В результате мы хотя бы вернулись к уровню модели 3, но она все еще далека от изначальной.
Model_8: увеличим количество эпох и чуть-чуть поправим параметры аугментации:

horizontal_flip = True,
width_shift_range = 0.1,
height_shift_range = 0.1,
zoom_range = [0.75, 1.25],
Model_9: уберем первый dense слой и batch normalization >> Accuracy увеличилась до 90.34%.

Model_10: попробуем понизить learning rate до 1e-5. Несмотря на то что точность снова ухудшилась до 76.99%, кажется что learning rate подобрана неплохо и мы можем взять эту модель за базовую и просто добавить большее количество эпох.

Model_11: попробуем перейти к тонкой настройке base model. Пробуем заморозить сеть и ее веса и для начала заморозим половину слоев. >> Получили accurcacy 77.03%. Если наш план сначала натренировать модель на 25-ти эпохах, а потом когда подберем лучший вариант добавить максимальное количество эпох, стоит ли полагаться на то что точность немного улучшилась на первых 25-ти эпохах?

Model_12: заморозим четверть весов, но пока не будем уменьшать learning rate до 1e-6.

Model_13: наоборот оставим размороженной четверть весов, а остальные три четверти заморозим.

Model_14: зморозим 300 первых слоев. >> Точность все равно немного меньше чем в случае 250 слоев, поэтому вернемся к этому параметру

Model_15: итак, мы готовы попробовать модель на 100 эпохах! Если результат будет не очень, вернемся к Модели_9. >> Результат получился 90.38%,что на небольшую долю лучше чем Модель 9, но все же мы потратили слишком много времени на это по сравнению с Моделью_9.

Model_16: вообще я вспомнила что в модели 10, на которой точность ухудшилась мы уменьшили learning rate, может быть если поэксперементировать с другими learning rate можно добиться лучшего результата. Попробуем ReduceLROnPlateau и вернемся к 25 эпохам для теста. >> Точность продолжает оставаться довольно низкой (75.96%)

Model_17: остается вопрос что лучше - Модель_9, которая показала точность 90% на 25ти эпохах, или надеяться что последняя модель если увеличить количество эпох покажет результат лучше? Думаю вернуться к Модели_9 и посмотреть что будет, если прогнать ее на 50ти эпохах. >> Получилась accuracy 89.91%, что даже немного меньше чем изначальная Модель_9.

Model_18: кажется, что нужно вернуться к предыдущим настройкам без заморозки части сети

Model_19: все тоже самое, но изменяем параметры в ReduceOnPlateau (эксперементируем с длительностью ожидания и фактором на который должна падать learning rate), добавляем TTA и увеличиваем количество эпох до 100. ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=8, min_lr=1e-5, verbose=1)

Model_20: увеличили размер картинки до 500 и поставила 100 эпох. 

Model_21: все тоже самое но только 5 эпох, чтобы успеть в недельную квоту. При таком количестве эпох никакие оптимизационные вещи по learning rate ReduceOnPlateau и тд просто не успевают срабатывать, и кажется единственное что сильно помогло увеличить качество модели это увеличение изображения.
